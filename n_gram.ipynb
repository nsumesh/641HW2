{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Sentences in each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing & adding sentence boundaries...\n",
      "Building vocabulary...\n",
      "Vocab size: 9950\n",
      "Replacing rare words with <UNK>...\n",
      "Saving processed files...\n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(s, replace_numbers = False):\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"…\", \"...\")\n",
    "    s = s.lower()\n",
    "    if replace_numbers:\n",
    "        s = re.sub(r\"\\d+([.,]\\d+)*\", \"<num>\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def load_and_normalize(filepath: Path, replace_numbers=False):\n",
    "    with filepath.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = [normalize_sentence(line, replace_numbers) for line in f if line.strip()]\n",
    "    # Add sentence boundary markers\n",
    "    lines = [f\"<s> {line} </s>\" for line in lines if line]\n",
    "    return lines\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in sentences:\n",
    "        for token in line.split():\n",
    "            counter[token] += 1\n",
    "    vocab = {tok for tok, c in counter.items() if c >= min_freq}\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "def replace_rare(sentences, vocab):\n",
    "    new_sentences = []\n",
    "    for line in sentences:\n",
    "        tokens = line.split()\n",
    "        new_line = \" \".join(tok if tok in vocab else \"<UNK>\" for tok in tokens)\n",
    "        new_sentences.append(new_line)\n",
    "    return new_sentences\n",
    "\n",
    "def save_sentences(sentences, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for line in sentences:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "base_raw = Path('/Users/nsumesh/Documents/GitHub/642HW2/ptbdataset')\n",
    "base_proc = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset')\n",
    "files = {\n",
    "    \"train\": \"ptb.train.txt\",\n",
    "    \"valid\": \"ptb.valid.txt\",\n",
    "    \"test\":  \"ptb.test.txt\"\n",
    "}\n",
    "\n",
    "print(\"Normalizing & adding sentence boundaries...\")\n",
    "train_sentences = load_and_normalize(base_raw / files[\"train\"], replace_numbers=True)\n",
    "valid_sentences = load_and_normalize(base_raw / files[\"valid\"], replace_numbers=True)\n",
    "test_sentences  = load_and_normalize(base_raw / files[\"test\"],  replace_numbers=True)\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "vocab, counter = build_vocab(train_sentences, min_freq=2)\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "print(\"Replacing rare words with <UNK>...\")\n",
    "train_sentences = replace_rare(train_sentences, vocab)\n",
    "valid_sentences = replace_rare(valid_sentences, vocab)\n",
    "test_sentences  = replace_rare(test_sentences, vocab)\n",
    "\n",
    "print(\"Saving processed files...\")\n",
    "save_sentences(train_sentences, base_proc / \"train.final.txt\")\n",
    "save_sentences(valid_sentences, base_proc / \"valid.final.txt\")\n",
    "save_sentences(test_sentences,  base_proc / \"test.final.txt\")\n",
    "\n",
    "with (base_proc / \"vocab.txt\").open(\"w\", encoding=\"utf-8\") as vf:\n",
    "    for token in sorted(vocab):\n",
    "        vf.write(token + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(path):\n",
    "    sents = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            sents.append(line.split())\n",
    "    return sents\n",
    "\n",
    "def build_n_gram_counts(sentences, n):\n",
    "    ngram_counts = Counter() \n",
    "    context_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        length = len(sentence)\n",
    "        if length<n:\n",
    "            continue\n",
    "        for i in range(length-n+1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            ngram_counts[ngram]+=1\n",
    "            if n>1:\n",
    "                context = ngram[:-1]\n",
    "                context_counts[context]+=1\n",
    "    return ngram_counts, context_counts\n",
    "\n",
    "def building_ngrams(sentences, max_n = 4):\n",
    "    results = {}\n",
    "    for i in range(1, max_n+1):\n",
    "        results[i] = build_n_gram_counts(sentences,i)\n",
    "    return results\n",
    "\n",
    "\n",
    "path_dir_training = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset/train.final.txt')\n",
    "path_dir_validation = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset/valid.final.txt')\n",
    "path_dir_test = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset/test.final.txt')\n",
    "training_sentences = read_sentences(path_dir_training)\n",
    "validation_sentences = read_sentences(path_dir_validation)\n",
    "testing_sentences = read_sentences(path_dir_test)\n",
    "all_counts = building_ngrams(training_sentences, max_n=4)\n",
    "unigram_count, unigram_context = all_counts[1]\n",
    "bigram_count, bigram_context = all_counts[2]\n",
    "trigram_count, trigram_context = all_counts[3]\n",
    "fourgram_count, fourgram_context = all_counts[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mle_probabilities(word, context, ngram_count, context_count):\n",
    "    ngram = context + (word,)\n",
    "    frequency = ngram_count.get(ngram,0)\n",
    "    if(len(context)==0):\n",
    "        denominator = sum(ngram_count.values())\n",
    "    else:\n",
    "        denominator = context_count.get(context,0)\n",
    "    if denominator==0.0 or frequency==0.0:\n",
    "        return 0.0\n",
    "    return frequency/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('the') = 0.05225094863722486\n",
      "P('market'|'the') = 0.00992712231632854\n"
     ]
    }
   ],
   "source": [
    "print(\"P('the') =\", mle_probabilities(\"the\", (), unigram_count, {}))\n",
    "print(\"P('market'|'the') =\", mle_probabilities(\"market\", (\"the\",), bigram_count, bigram_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity:  616.5898425739796\n",
      "Bigram Perplexity:  inf\n",
      "Trigram Perplexity:  inf\n",
      "Four gram Perplexity:  inf\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(sentences, ngram_count, context_count, n):\n",
    "    total_log_probabilities = 0.0\n",
    "    tokens = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(n-1, len(sentence)):\n",
    "            context = tuple(sentence[i:i+n]) if n>1 else ()\n",
    "            word = sentence[i]\n",
    "            mle_probability = mle_probabilities(word, context, ngram_count, context_count)\n",
    "            if mle_probability==0.0:\n",
    "                return float(\"inf\")\n",
    "            total_log_probabilities+=math.log2(mle_probability)\n",
    "            tokens+=1\n",
    "    if tokens==0.0:\n",
    "        return float(\"inf\")\n",
    "    average_log_probabilities = total_log_probabilities/tokens\n",
    "    return 2**(-average_log_probabilities)\n",
    "\n",
    "unigram_perplexity = compute_perplexity(training_sentences, unigram_count, unigram_context, n=1)\n",
    "bigram_perplexity = compute_perplexity(training_sentences, bigram_count, bigram_context, n=2)\n",
    "trigram_perplexity = compute_perplexity(training_sentences, trigram_count, trigram_context, n=3)\n",
    "fourgram_perplexity = compute_perplexity(training_sentences, fourgram_count, fourgram_context, n=4)\n",
    "\n",
    "print(\"Unigram Perplexity: \", unigram_perplexity)\n",
    "print(\"Bigram Perplexity: \", bigram_perplexity)\n",
    "print(\"Trigram Perplexity: \", trigram_perplexity)\n",
    "print(\"Four gram Perplexity: \", fourgram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity with Laplace:  616.7887358737605\n",
      "Bigram Perplexity with Laplace:  9949.999998582753\n",
      "Trigram Perplexity with Laplace:  9949.999998779924\n",
      "Fourgram Perplexity with Laplace:  9949.99999899671\n"
     ]
    }
   ],
   "source": [
    "def laplace_probability(word, context, ngram_count, context_count):\n",
    "    ngram = context + (word,)\n",
    "    frequency = ngram_count.get(ngram,0)+1\n",
    "    if(len(context)==0):\n",
    "        denominator = sum(ngram_count.values()) + len(vocab)\n",
    "    else:\n",
    "        denominator = context_count.get(context,0) + len(vocab)\n",
    "    return frequency/denominator \n",
    "\n",
    "def compute_perplexity_with_laplace(sentences, ngram_count, context_count, n):\n",
    "    total_log_probabilities = 0.0\n",
    "    tokens = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(n-1, len(sentence)):\n",
    "            context = tuple(sentence[i:i+n]) if n>1 else ()\n",
    "            word = sentence[i]\n",
    "            laplace_prob = laplace_probability(word, context, ngram_count, context_count)\n",
    "            if laplace_prob==0.0:\n",
    "                return float(\"inf\")\n",
    "            total_log_probabilities+=math.log2(laplace_prob)\n",
    "            tokens+=1\n",
    "    if tokens==0.0:\n",
    "        return float(\"inf\")\n",
    "    average_log_probabilities = total_log_probabilities/tokens\n",
    "    return 2**(-average_log_probabilities)\n",
    "\n",
    "unigram_perplexity_with_laplace = compute_perplexity_with_laplace(training_sentences, unigram_count, unigram_context, n=1)\n",
    "bigram_perplexity_with_laplace = compute_perplexity_with_laplace(training_sentences, bigram_count, bigram_context, n=2)\n",
    "trigram_perplexity_with_laplace = compute_perplexity_with_laplace(training_sentences, trigram_count, trigram_context, n=3)\n",
    "fourgram_perplexity_with_laplace = compute_perplexity_with_laplace(training_sentences, fourgram_count, fourgram_context, n=4)\n",
    "\n",
    "print(\"Unigram Perplexity with Laplace: \", unigram_perplexity_with_laplace)\n",
    "print(\"Bigram Perplexity with Laplace: \", bigram_perplexity_with_laplace)\n",
    "print(\"Trigram Perplexity with Laplace: \", trigram_perplexity_with_laplace)\n",
    "print(\"Fourgram Perplexity with Laplace: \", fourgram_perplexity_with_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: [0.4, 0.2, 0.2, 0.2]\n",
      "Lowest Perplexity: 669.6562359849099\n"
     ]
    }
   ],
   "source": [
    "def interpolation(word, context, unigram_count, bigram_count, bigram_context, trigram_count, trigram_context, fourgram_count, fourgram_context, lambdas, vocab_size):\n",
    "    lambda1, lambda2, lambda3, lambda4 = lambdas[0], lambdas[1], lambdas[2], lambdas[3]\n",
    "    unigram_probability = (unigram_count.get((word,), 0) + 1) / (sum(unigram_count.values()) + vocab_size)\n",
    "    if len(context) >= 1:\n",
    "        context_bi = (context[-1],)\n",
    "        bigram_probability = (bigram_count.get(context_bi + (word,), 0) + 1) / (bigram_context.get(context_bi, 0) + vocab_size)\n",
    "    else:\n",
    "        bigram_probability = unigram_probability\n",
    "    if len(context) >= 2:\n",
    "        context_tri = tuple(context[-2:])\n",
    "        trigram_probability = (trigram_count.get(context_tri + (word,), 0) + 1) / (trigram_context.get(context_tri, 0) + vocab_size)\n",
    "    else:\n",
    "        trigram_probability = bigram_probability\n",
    "    if len(context) >= 3:\n",
    "        context_four = tuple(context[-3:])\n",
    "        fourgram_probability = (fourgram_count.get(context_four + (word,), 0) + 1) / (fourgram_context.get(context_four, 0) + vocab_size)\n",
    "    else:\n",
    "        fourgram_probability = trigram_probability\n",
    "    return lambda1 * unigram_probability + lambda2 * bigram_probability + lambda3 * trigram_probability + lambda4 * fourgram_probability\n",
    "\n",
    "\n",
    "def interpolation_perplexity(sentences, unigram_count, bigram_count, bigram_context, trigram_count, trigram_context, fourgram_count, fourgram_context, lambdas, vocab_size):\n",
    "    total_log_probabilities = 0.0\n",
    "    tokens = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(3, len(sentence)):  \n",
    "            context = tuple(sentence[i-3:i])\n",
    "            probability = interpolation(sentence[i], context,\n",
    "                                        unigram_count, bigram_count, bigram_context,\n",
    "                                        trigram_count, trigram_context,\n",
    "                                        fourgram_count, fourgram_context,\n",
    "                                        lambdas, vocab_size)\n",
    "            if probability <= 0:\n",
    "                continue\n",
    "            total_log_probabilities += math.log2(probability)\n",
    "            tokens += 1\n",
    "    average_log_probabilities = total_log_probabilities / tokens\n",
    "    return 2 ** (-average_log_probabilities)\n",
    "\n",
    "lambda_weights = [\n",
    "    [0.10, 0.15, 0.35, 0.40],\n",
    "    [0.05, 0.05, 0.30, 0.60],\n",
    "    [0.25, 0.10, 0.50, 0.15],\n",
    "    [0.40, 0.20, 0.20, 0.20],\n",
    "    [0.01, 0.24, 0.25, 0.50],\n",
    "    [0.33, 0.17, 0.25, 0.25],\n",
    "    [0.12, 0.18, 0.27, 0.43],\n",
    "    [0.07, 0.28, 0.10, 0.55],\n",
    "    [0.22, 0.22, 0.22, 0.34],\n",
    "    [0.48, 0.12, 0.30, 0.10],\n",
    "    [0.26, 0.14, 0.31, 0.29],\n",
    "    [0.09, 0.41, 0.19, 0.31]\n",
    "]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "highest_perplexity = float(\"inf\")\n",
    "best_lambda = None\n",
    "\n",
    "for lambda_w in lambda_weights:\n",
    "    interpolation_perplexity_prob = interpolation_perplexity(validation_sentences, unigram_count, bigram_count, bigram_context, trigram_count, trigram_context, fourgram_count, fourgram_context, lambda_w, vocab_size)\n",
    "    if interpolation_perplexity_prob < highest_perplexity:\n",
    "        highest_perplexity, best_lambda = interpolation_perplexity_prob, lambda_w\n",
    "\n",
    "print(\"Best Lambda:\", best_lambda)\n",
    "print(\"Lowest Perplexity:\", highest_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff Perplexity: 373.05959789616816\n"
     ]
    }
   ],
   "source": [
    "def backoff_probability(word, context,\n",
    "                        unigram_count, bigram_count, bigram_context,\n",
    "                        trigram_count, trigram_context,\n",
    "                        fourgram_count, fourgram_context, alpha=0.4):\n",
    "    if len(context) >= 3:\n",
    "        context_four = tuple(context[-3:])\n",
    "        fourgram_freq = fourgram_count.get(context_four + (word,), 0)\n",
    "        if fourgram_freq > 0:\n",
    "            return fourgram_freq / fourgram_context.get(context_four, 1)\n",
    "        return alpha * backoff_probability(word, context[-2:],\n",
    "                                           unigram_count, bigram_count, bigram_context,\n",
    "                                           trigram_count, trigram_context,\n",
    "                                           fourgram_count, fourgram_context, alpha=alpha)\n",
    "\n",
    "    if len(context) >= 2:\n",
    "        context_tri = tuple(context[-2:])\n",
    "        trigram_freq = trigram_count.get(context_tri + (word,), 0)\n",
    "        if trigram_freq > 0:\n",
    "            return trigram_freq / trigram_context.get(context_tri, 1)\n",
    "        return alpha * backoff_probability(word, context[-1:],\n",
    "                                           unigram_count, bigram_count, bigram_context,\n",
    "                                           trigram_count, trigram_context,\n",
    "                                           fourgram_count, fourgram_context, alpha=alpha)\n",
    "\n",
    "    if len(context) >= 1:\n",
    "        context_bi = (context[-1],)\n",
    "        bigram_freq = bigram_count.get(context_bi + (word,), 0)\n",
    "        if bigram_freq > 0:\n",
    "            return bigram_freq / bigram_context.get(context_bi, 1)\n",
    "        return alpha * backoff_probability(word, (),unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context, alpha=alpha)\n",
    "\n",
    "    total = sum(unigram_count.values())\n",
    "    return unigram_count.get((word,), 0) / total\n",
    "\n",
    "\n",
    "def backoff_perplexity(sentences,unigram_count, bigram_count, bigram_context, trigram_count, trigram_context, fourgram_count, fourgram_context, alpha=0.4):\n",
    "    total_log_probabilities, tokens = 0.0, 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(3, len(sentence)):\n",
    "            context = tuple(sentence[i - 3:i])\n",
    "            probability = backoff_probability(sentence[i], context,unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha)\n",
    "            if probability == 0.0:\n",
    "                continue\n",
    "            total_log_probabilities += math.log2(probability)\n",
    "            tokens += 1\n",
    "    if tokens == 0:\n",
    "        return float(\"inf\")\n",
    "    avg_log_probability = total_log_probabilities / tokens\n",
    "    # FIX: remove the negative sign — perplexity must be positive\n",
    "    return 2 ** (-avg_log_probability)\n",
    "\n",
    "alpha = 0.4\n",
    "boff_perplexity = backoff_perplexity(validation_sentences,unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha)\n",
    "print(\"Backoff Perplexity:\", boff_perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Perplexity on Testing Set:  inf\n",
      "Unigram Laplace Perplexity on Testing Set:  576.5185854219595\n",
      "Bigram Laplace Perplexity on Testing Set:  9950.000000006661\n",
      "Trigram Laplace Perplexity on Testing Set:  9950.000000040365\n",
      "Fourgram Laplace Perplexity on Testing Set:  9950.000000040218\n",
      "Interpolation perplexity on Testing Set:  662.0366799344446\n",
      "Backoff Perplexity for Testing set:  358.8614828082989\n"
     ]
    }
   ],
   "source": [
    "mle_perplexity = compute_perplexity(test_sentences, unigram_count, unigram_context, n=1)\n",
    "test_unigram_laplace_perplexity = compute_perplexity_with_laplace(testing_sentences, unigram_count, unigram_context, n=1)\n",
    "test_bigram_laplace_perplexity = compute_perplexity_with_laplace(testing_sentences, bigram_count, bigram_context, n=2)\n",
    "test_trigram_laplace_perplexity = compute_perplexity_with_laplace(testing_sentences, trigram_count, trigram_context, n=3)\n",
    "test_fourgram_laplace_perplexity = compute_perplexity_with_laplace(testing_sentences, fourgram_count, fourgram_context, n=4)\n",
    "lambdas = [0.4, 0.2, 0.2, 0.2]\n",
    "test_interpolation_perplexity = interpolation_perplexity(testing_sentences, unigram_count, bigram_count, bigram_context, trigram_count, trigram_context, fourgram_count, fourgram_context, lambdas, vocab_size)\n",
    "test_backoff_perplexity = backoff_perplexity(testing_sentences, unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha=0.4)\n",
    "\n",
    "print('MLE Perplexity on Testing Set: ',mle_perplexity)\n",
    "print('Unigram Laplace Perplexity on Testing Set: ',test_unigram_laplace_perplexity)\n",
    "print('Bigram Laplace Perplexity on Testing Set: ',test_bigram_laplace_perplexity)\n",
    "print('Trigram Laplace Perplexity on Testing Set: ',test_trigram_laplace_perplexity)\n",
    "print('Fourgram Laplace Perplexity on Testing Set: ',test_fourgram_laplace_perplexity)\n",
    "print('Interpolation perplexity on Testing Set: ',test_interpolation_perplexity)\n",
    "print('Backoff Perplexity for Testing set: ', test_backoff_perplexity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Sentences function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Generated:  younger bought for nancy reagan was pulled into some lung cancers as of june department\n",
      "Sentence Generated:  punish him for crossing a study why the situation did n't reach many small communities\n",
      "Sentence Generated:  nursing home patients apt to be named a number later pro-life converting a professional baseball\n",
      "Sentence Generated:  floor at about a fourth of all u.s. beer sales and is expected </s> profit\n",
      "Sentence Generated:  stuff </s> it </s> draft agreements provided the state department but the implications could be\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentences(unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha=0.4):\n",
    "    start_token = [random.choice(list(unigram_count.keys()))[0]]\n",
    "    sentence = list(start_token)\n",
    "    for _ in range(15-len(start_token)):\n",
    "        context = tuple(sentence[-3:])\n",
    "        candidates = []\n",
    "        probs = []\n",
    "        for word in vocab:\n",
    "            if word in ['<s>','<\\s>','<unk>']:\n",
    "                continue\n",
    "            prob = backoff_probability(word, context, unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha=0.4)\n",
    "            if prob>0:\n",
    "                candidates.append(word)\n",
    "                probs.append(prob)\n",
    "        if not candidates:\n",
    "            break\n",
    "        word = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        sentence.append(word)\n",
    "        if word in ['.','!','?']:\n",
    "            break\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "for i in range(5):\n",
    "    sentence = generate_sentences(unigram_count, bigram_count, bigram_context,trigram_count, trigram_context,fourgram_count, fourgram_context,alpha=0.4)\n",
    "    print(\"Sentence Generated: \",sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
