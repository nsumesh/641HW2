{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Sentences in each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing & adding sentence boundaries...\n",
      "Building vocabulary...\n",
      "Vocab size: 9950\n",
      "Replacing rare words with <UNK>...\n",
      "Saving processed files...\n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(s, replace_numbers = False):\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"…\", \"...\")\n",
    "    s = s.lower()\n",
    "    if replace_numbers:\n",
    "        s = re.sub(r\"\\d+([.,]\\d+)*\", \"<num>\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def load_and_normalize(filepath: Path, replace_numbers=False):\n",
    "    with filepath.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = [normalize_sentence(line, replace_numbers) for line in f if line.strip()]\n",
    "    # Add sentence boundary markers\n",
    "    lines = [f\"<s> {line} </s>\" for line in lines if line]\n",
    "    return lines\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in sentences:\n",
    "        for token in line.split():\n",
    "            counter[token] += 1\n",
    "    vocab = {tok for tok, c in counter.items() if c >= min_freq}\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "def replace_rare(sentences, vocab):\n",
    "    new_sentences = []\n",
    "    for line in sentences:\n",
    "        tokens = line.split()\n",
    "        new_line = \" \".join(tok if tok in vocab else \"<UNK>\" for tok in tokens)\n",
    "        new_sentences.append(new_line)\n",
    "    return new_sentences\n",
    "\n",
    "def save_sentences(sentences, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for line in sentences:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "base_raw = Path('/Users/nsumesh/Documents/GitHub/642HW2/ptbdataset')\n",
    "base_proc = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset')\n",
    "files = {\n",
    "    \"train\": \"ptb.train.txt\",\n",
    "    \"valid\": \"ptb.valid.txt\",\n",
    "    \"test\":  \"ptb.test.txt\"\n",
    "}\n",
    "\n",
    "print(\"Normalizing & adding sentence boundaries...\")\n",
    "train_sentences = load_and_normalize(base_raw / files[\"train\"], replace_numbers=True)\n",
    "valid_sentences = load_and_normalize(base_raw / files[\"valid\"], replace_numbers=True)\n",
    "test_sentences  = load_and_normalize(base_raw / files[\"test\"],  replace_numbers=True)\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "vocab, counter = build_vocab(train_sentences, min_freq=2)\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "print(\"Replacing rare words with <UNK>...\")\n",
    "train_sentences = replace_rare(train_sentences, vocab)\n",
    "valid_sentences = replace_rare(valid_sentences, vocab)\n",
    "test_sentences  = replace_rare(test_sentences, vocab)\n",
    "\n",
    "print(\"Saving processed files...\")\n",
    "save_sentences(train_sentences, base_proc / \"train.final.txt\")\n",
    "save_sentences(valid_sentences, base_proc / \"valid.final.txt\")\n",
    "save_sentences(test_sentences,  base_proc / \"test.final.txt\")\n",
    "\n",
    "with (base_proc / \"vocab.txt\").open(\"w\", encoding=\"utf-8\") as vf:\n",
    "    for token in sorted(vocab):\n",
    "        vf.write(token + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(path):\n",
    "    sents = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            sents.append(line.split())\n",
    "    return sents\n",
    "\n",
    "def build_n_gram_counts(sentences, n):\n",
    "    ngram_counts = Counter() \n",
    "    context_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        length = len(sentence)\n",
    "        if length<n:\n",
    "            continue\n",
    "        for i in range(length-n+1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            ngram_counts[ngram]+=1\n",
    "            if n>1:\n",
    "                context = ngram[:-1]\n",
    "                context_counts[context]+=1\n",
    "    return ngram_counts, context_counts\n",
    "\n",
    "def building_ngrams(sentences, max_n = 4):\n",
    "    results = {}\n",
    "    for i in range(1, max_n+1):\n",
    "        results[i] = build_n_gram_counts(sentences,i)\n",
    "    return results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
