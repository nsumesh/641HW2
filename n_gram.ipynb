{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Sentences in each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing & adding sentence boundaries...\n",
      "Building vocabulary...\n",
      "Vocab size: 9950\n",
      "Replacing rare words with <UNK>...\n",
      "Saving processed files...\n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(s, replace_numbers = False):\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"…\", \"...\")\n",
    "    s = s.lower()\n",
    "    if replace_numbers:\n",
    "        s = re.sub(r\"\\d+([.,]\\d+)*\", \"<num>\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def load_and_normalize(filepath: Path, replace_numbers=False):\n",
    "    with filepath.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = [normalize_sentence(line, replace_numbers) for line in f if line.strip()]\n",
    "    # Add sentence boundary markers\n",
    "    lines = [f\"<s> {line} </s>\" for line in lines if line]\n",
    "    return lines\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in sentences:\n",
    "        for token in line.split():\n",
    "            counter[token] += 1\n",
    "    vocab = {tok for tok, c in counter.items() if c >= min_freq}\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "def replace_rare(sentences, vocab):\n",
    "    new_sentences = []\n",
    "    for line in sentences:\n",
    "        tokens = line.split()\n",
    "        new_line = \" \".join(tok if tok in vocab else \"<UNK>\" for tok in tokens)\n",
    "        new_sentences.append(new_line)\n",
    "    return new_sentences\n",
    "\n",
    "def save_sentences(sentences, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for line in sentences:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "base_raw = Path('/Users/nsumesh/Documents/GitHub/642HW2/ptbdataset')\n",
    "base_proc = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset')\n",
    "files = {\n",
    "    \"train\": \"ptb.train.txt\",\n",
    "    \"valid\": \"ptb.valid.txt\",\n",
    "    \"test\":  \"ptb.test.txt\"\n",
    "}\n",
    "\n",
    "print(\"Normalizing & adding sentence boundaries...\")\n",
    "train_sentences = load_and_normalize(base_raw / files[\"train\"], replace_numbers=True)\n",
    "valid_sentences = load_and_normalize(base_raw / files[\"valid\"], replace_numbers=True)\n",
    "test_sentences  = load_and_normalize(base_raw / files[\"test\"],  replace_numbers=True)\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "vocab, counter = build_vocab(train_sentences, min_freq=2)\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "print(\"Replacing rare words with <UNK>...\")\n",
    "train_sentences = replace_rare(train_sentences, vocab)\n",
    "valid_sentences = replace_rare(valid_sentences, vocab)\n",
    "test_sentences  = replace_rare(test_sentences, vocab)\n",
    "\n",
    "print(\"Saving processed files...\")\n",
    "save_sentences(train_sentences, base_proc / \"train.final.txt\")\n",
    "save_sentences(valid_sentences, base_proc / \"valid.final.txt\")\n",
    "save_sentences(test_sentences,  base_proc / \"test.final.txt\")\n",
    "\n",
    "with (base_proc / \"vocab.txt\").open(\"w\", encoding=\"utf-8\") as vf:\n",
    "    for token in sorted(vocab):\n",
    "        vf.write(token + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(path):\n",
    "    sents = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            sents.append(line.split())\n",
    "    return sents\n",
    "\n",
    "def build_n_gram_counts(sentences, n):\n",
    "    ngram_counts = Counter() \n",
    "    context_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        length = len(sentence)\n",
    "        if length<n:\n",
    "            continue\n",
    "        for i in range(length-n+1):\n",
    "            ngram = tuple(sentence[i:i+n])\n",
    "            ngram_counts[ngram]+=1\n",
    "            if n>1:\n",
    "                context = ngram[:-1]\n",
    "                context_counts[context]+=1\n",
    "    return ngram_counts, context_counts\n",
    "\n",
    "def building_ngrams(sentences, max_n = 4):\n",
    "    results = {}\n",
    "    for i in range(1, max_n+1):\n",
    "        results[i] = build_n_gram_counts(sentences,i)\n",
    "    return results\n",
    "\n",
    "\n",
    "path_dir = Path('/Users/nsumesh/Documents/GitHub/642HW2/normalizedptbdataset/train.final.txt')\n",
    "sentences = read_sentences(path_dir)\n",
    "\n",
    "all_counts = building_ngrams(sentences, max_n=4)\n",
    "unigram_count, unigram_context = all_counts[1]\n",
    "bigram_count, bigram_context = all_counts[2]\n",
    "trigram_count, trigram_context = all_counts[3]\n",
    "fourgram_count, fourgram_context = all_counts[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mle_probabilities(word, context, ngram_count, context_count):\n",
    "    ngram = context + (word,)\n",
    "    frequency = ngram_count.get(ngram,0)\n",
    "    if(len(context)==0):\n",
    "        denominator = sum(ngram_count.values())\n",
    "    else:\n",
    "        denominator = context_count.get(context,0)\n",
    "    if denominator==0.0 or frequency==0.0:\n",
    "        return 0.0\n",
    "    return frequency/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('the') = 0.05225094863722486\n",
      "P('market'|'the') = 0.00992712231632854\n"
     ]
    }
   ],
   "source": [
    "print(\"P('the') =\", mle_probabilities(\"the\", (), unigram_count, {}))\n",
    "print(\"P('market'|'the') =\", mle_probabilities(\"market\", (\"the\",), bigram_count, bigram_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity:  616.5898425739796\n",
      "Bigram Perplexity:  inf\n",
      "Trigram Perplexity:  inf\n",
      "Four gram Perplexity:  inf\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(sentences, ngram_count, context_count, n):\n",
    "    total_log_probabilities = 0.0\n",
    "    tokens = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(n-1, len(sentence)):\n",
    "            context = tuple(sentence[i:i+n]) if n>1 else ()\n",
    "            word = sentence[i]\n",
    "            mle_probability = mle_probabilities(word, context, ngram_count, context_count)\n",
    "            if mle_probability==0.0:\n",
    "                return float(\"inf\")\n",
    "            total_log_probabilities+=math.log2(mle_probability)\n",
    "            tokens+=1\n",
    "    if tokens==0.0:\n",
    "        return float(\"inf\")\n",
    "    average_log_probabilities = total_log_probabilities/tokens\n",
    "    return 2**(-average_log_probabilities)\n",
    "\n",
    "unigram_perplexity = compute_perplexity(sentences, unigram_count, unigram_context, n=1)\n",
    "bigram_perplexity = compute_perplexity(sentences, bigram_count, bigram_context, n=2)\n",
    "trigram_perplexity = compute_perplexity(sentences, trigram_count, trigram_context, n=3)\n",
    "fourgram_perplexity = compute_perplexity(sentences, fourgram_count, fourgram_context, n=4)\n",
    "\n",
    "print(\"Unigram Perplexity: \", unigram_perplexity)\n",
    "print(\"Bigram Perplexity: \", bigram_perplexity)\n",
    "print(\"Trigram Perplexity: \", trigram_perplexity)\n",
    "print(\"Four gram Perplexity: \", fourgram_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity with Laplace:  616.7887358737605\n",
      "Bigram Perplexity with Laplace:  9949.999998582753\n",
      "Trigram Perplexity with Laplace:  9949.999998779924\n",
      "Fourgram Perplexity with Laplace:  9949.99999899671\n"
     ]
    }
   ],
   "source": [
    "def laplace_probability(word, context, ngram_count, context_count):\n",
    "    ngram = context + (word,)\n",
    "    frequency = ngram_count.get(ngram,0)+1\n",
    "    if(len(context)==0):\n",
    "        denominator = sum(ngram_count.values()) + len(vocab)\n",
    "    else:\n",
    "        denominator = context_count.get(context,0) + len(vocab)\n",
    "    # if denominator==0.0 or frequency==0.0:\n",
    "    #     return 0.0\n",
    "    return frequency/denominator \n",
    "\n",
    "\n",
    "def compute_perplexity_with_laplace(sentences, ngram_count, context_count, n):\n",
    "    total_log_probabilities = 0.0\n",
    "    tokens = 0\n",
    "    for sentence in sentences:\n",
    "        for i in range(n-1, len(sentence)):\n",
    "            context = tuple(sentence[i:i+n]) if n>1 else ()\n",
    "            word = sentence[i]\n",
    "            laplace_prob = laplace_probability(word, context, ngram_count, context_count)\n",
    "            if laplace_prob==0.0:\n",
    "                return float(\"inf\")\n",
    "            total_log_probabilities+=math.log2(laplace_prob)\n",
    "            tokens+=1\n",
    "    if tokens==0.0:\n",
    "        return float(\"inf\")\n",
    "    average_log_probabilities = total_log_probabilities/tokens\n",
    "    return 2**(-average_log_probabilities)\n",
    "\n",
    "\n",
    "unigram_perplexity_with_laplace = compute_perplexity_with_laplace(sentences, unigram_count, unigram_context, n=1)\n",
    "bigram_perplexity_with_laplace = compute_perplexity_with_laplace(sentences, bigram_count, bigram_context, n=2)\n",
    "trigram_perplexity_with_laplace = compute_perplexity_with_laplace(sentences, trigram_count, trigram_context, n=3)\n",
    "fourgram_perplexity_with_laplace = compute_perplexity_with_laplace(sentences, fourgram_count, fourgram_context, n=4)\n",
    "\n",
    "print(\"Unigram Perplexity with Laplace: \", unigram_perplexity_with_laplace)\n",
    "print(\"Bigram Perplexity with Laplace: \", bigram_perplexity_with_laplace)\n",
    "print(\"Trigram Perplexity with Laplace: \", trigram_perplexity_with_laplace)\n",
    "print(\"Fourgram Perplexity with Laplace: \", fourgram_perplexity_with_laplace)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
